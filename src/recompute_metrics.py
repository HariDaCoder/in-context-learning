"""
Script Ä‘á»ƒ cháº¡y láº¡i evaluation/metrics cho cÃ¡c experiments Ä‘Ã£ train xong.
Chá»‰ cáº§n thÃªm run_path vÃ o danh sÃ¡ch RUN_PATHS vÃ  cháº¡y script.
"""

import os
from pathlib import Path
from eval import get_run_metrics

# ÄÆ°á»ng dáº«n project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent
MODELS_DIR = PROJECT_ROOT / "models"

# =============================================================================
# DANH SÃCH CÃC RUN Cáº¦N TÃNH Láº I METRICS
# =============================================================================
# ThÃªm tÃªn folder cá»§a experiment vÃ o Ä‘Ã¢y (tÃªn folder trong models/)
# VÃ­ dá»¥: "fig3_noise_bernoulli_p0.3", "fig1_exp_w_rate0.5", etc.

RUN_PATHS = [
    # ThÃªm tÃªn experiments vÃ o Ä‘Ã¢y:
    "fig3_noise_bernoulli_p0.3",
    "fig3_noise_gamma_k4.0_lambda1.0",
    "fig3_noise_poisson_lambda2.0",
    # "fig3_noise_poisson_lambda3.0",
    # "fig3_noise_t-student_df3.0",
]

# =============================================================================
# Cáº¤U HÃŒNH
# =============================================================================
FORCE_RECOMPUTE = True  # True = luÃ´n tÃ­nh láº¡i, False = chá»‰ tÃ­nh náº¿u chÆ°a cÃ³ hoáº·c cÅ©
SKIP_BASELINES = True   # True = chá»‰ tÃ­nh cho model chÃ­nh, False = tÃ­nh cáº£ baselines


def recompute_single_run(run_name, force=True, skip_baselines=True):
    """Cháº¡y láº¡i metrics cho má»™t run"""
    run_path = MODELS_DIR / run_name
    
    if not run_path.exists():
        print(f"âŒ KHÃ”NG TÃŒM THáº¤Y: {run_path}")
        return False
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Äang tÃ­nh metrics cho: {run_name}")
    print(f"{'='*70}")
    
    try:
        # XÃ³a file metrics cÅ© náº¿u force recompute
        metrics_file = run_path / "metrics.json"
        if force and metrics_file.exists():
            print(f"ğŸ—‘ï¸  XÃ³a metrics cÅ©...")
            metrics_file.unlink()
        
        # Cháº¡y evaluation
        metrics = get_run_metrics(
            str(run_path),
            step=-1,
            cache=True,
            skip_model_load=False,
            skip_baselines=skip_baselines
        )
        
        print(f"âœ… THÃ€NH CÃ”NG: {run_name}")
        print(f"ğŸ“ Metrics saved to: {metrics_file}")
        return True
        
    except Exception as e:
        print(f"âŒ Lá»–I khi tÃ­nh metrics cho {run_name}:")
        print(f"   {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print(f"\n{'#'*70}")
    print(f"ğŸ”„ RECOMPUTE METRICS")
    print(f"ğŸ“‚ Models directory: {MODELS_DIR}")
    print(f"ğŸ“Š Total runs to process: {len(RUN_PATHS)}")
    print(f"âš™ï¸  Force recompute: {FORCE_RECOMPUTE}")
    print(f"âš™ï¸  Skip baselines: {SKIP_BASELINES}")
    print(f"{'#'*70}\n")
    
    if not RUN_PATHS:
        print("âš ï¸  Danh sÃ¡ch RUN_PATHS trá»‘ng!")
        print("   Vui lÃ²ng thÃªm tÃªn experiments vÃ o RUN_PATHS trong file nÃ y.")
        return
    
    success_count = 0
    failed_runs = []
    
    for i, run_name in enumerate(RUN_PATHS, 1):
        print(f"\n[{i}/{len(RUN_PATHS)}] Processing: {run_name}")
        
        success = recompute_single_run(
            run_name,
            force=FORCE_RECOMPUTE,
            skip_baselines=SKIP_BASELINES
        )
        
        if success:
            success_count += 1
        else:
            failed_runs.append(run_name)
    
    # Tá»•ng káº¿t
    print(f"\n{'#'*70}")
    print(f"âœ… HOÃ€N THÃ€NH!")
    print(f"ğŸ“Š ThÃ nh cÃ´ng: {success_count}/{len(RUN_PATHS)}")
    if failed_runs:
        print(f"âŒ Tháº¥t báº¡i: {len(failed_runs)}")
        print(f"   Runs failed:")
        for run in failed_runs:
            print(f"   - {run}")
    print(f"{'#'*70}\n")


if __name__ == "__main__":
    main()
