model:
  family: gpt2
  n_dims: 5
  n_embd: 256
  n_head: 8
  n_layer: 12
  n_positions: 11

training:
  task: linear_regression
  data: gaussian
  task_kwargs: {}
  data_kwargs: {}
  batch_size: 64
  learning_rate: 0.0001
  save_every_steps: 1000
  keep_every_steps: 100000
  train_steps: 50001
  num_tasks: 10000
  num_training_examples: null
  curriculum:
    dims:
      start: 5
      end: 5
      inc: 1
      interval: 2000
    points:
      start: 11
      end: 11
      inc: 2
      interval: 2000

out_dir: ../models/linear_regression

wandb:
  entity: haitrinh220970-hcmut
  log_every_steps: 100
  name: test_01x
  notes: ''
  project: in-context-training